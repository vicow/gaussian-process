{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidekick - Formal Definition of the Problems\n",
    "\n",
    "## Notations\n",
    " - Let us denote $\\mathbf{v}_{i:j}$, $i \\leq j$ the subvector $\\mathbf{u} = [v_i, ..., v_j]^T$ consisting of the elements $v_i$ to $v_j$ of the vector $\\mathbf{v}$.\n",
    "\n",
    " - Let us denote $(v_{i:j}, i \\leq j)$ the subsequence $(u_n, n = i, ..., j) = (v_i, ..., v_j)$ consisting of the elements $v_i$ to $v_j$ of the sequence $(v_n, n = 1, ..., N)$.\n",
    "\n",
    "## Problems\n",
    "\n",
    "### Single-Project Regression\n",
    "#### Model\n",
    "We are approaching the problem as time series regression, considering only one project. Our dataset $\\mathcal{D} = \\left\\{ (x_i, y_i) \\mid i = 1, ..., T \\right\\}$ consists of $T$ observations, with $x_i$ the time index of the amount of money $y_i$. Hence, we have $X = [1, ..., T]^T$ a $(T \\times 1)$ matrix of time indices and $\\mathbf{y} = [y_1, ..., y_T]^T$ a vector of observed values. We model the pledged money $f(\\mathbf{x})$ at time indices $\\mathbf{x}$ as a Gaussian Process:\n",
    "\n",
    "$$f(\\mathbf{x}) \\sim GP \\left( m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}') \\right). $$\n",
    "\n",
    "Our goal is to predict the future values of the pledged money $\\mathbf{f}_* = \\mathbf{f}_{t:T} = f(X_{t:T})$ at future time indices $X_* = X_{t:T} = [t, ..., T]^T$ after observing the values $\\mathbf{y} = \\mathbf{y}_{1:t} = [y_1, ..., y_t]^T$ at time indices $X = X_{1:t} = [1, ..., t]^T$. In the GP framework, we can compute this prediction using\n",
    "\n",
    "$$\\mathbf{f}_* \\mid X, \\mathbf{y}, X_* \\sim \\mathcal{N}\\left(\\overline{\\mathbf{f}}_*, \\text{ cov}(\\mathbf{f}_*)  \\right) \\\\\n",
    "\\overline{\\mathbf{f}}_* = K(X_*, X) \\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}\\mathbf{y} \\\\\n",
    "\\text{cov}(\\mathbf{f}_*) = K(X_*, X_*) - K(X_*, X)\\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}K(X, X_*).\n",
    "$$ \n",
    "\n",
    "Finally, the kernel's (hyper)parameters $\\theta_*$ are learned by maximizing the *log marginal likehood*\n",
    "\n",
    "$$\\theta_* = \\underset{\\theta} {\\arg\\max} \\log p(\\mathbf{y} \\mid X, \\theta) = \\underset{\\theta} {\\arg\\max} \\left\\{ -\\frac{1}{2}\\mathbf{y}^T \\left[ K+ \\sigma_n^2I \\right]^{-1}\\mathbf{y} -\\frac{1}{2}\\log det\\left[K+ \\sigma_n^2I\\right] -\\frac{T}{2}\\log 2\\pi \\right\\},$$\n",
    "\n",
    "with $K = K(X, X)$.\n",
    "\n",
    "#### Results\n",
    "The major problem in this context was that the predictive mean $\\mathbf{f}_*$ always falls back to the mean $m(\\mathbf{x})$ very quickly. One solution has been to combine two squared-exponential kernels and initializing one of them to a large length-scale in order to capture the global trend. This yields to some reasonable result. However, when applying the same model to another ones ($\\theta$ learned over one project and used to predict another) gives very poor performance.\n",
    "\n",
    "\n",
    "### Multi-Project Regression\n",
    "#### Model\n",
    "Hence, our next idea is to consider $P$ projects at the same time and try to learn the hyperparameters $\\theta$ over various time series. For a given project $p$, we have the following dataset $\\mathcal{D}^{(p)} = \\left\\{ (x_i^{(p)}, y_i^{(p)}) \\mid i = 1, ..., T \\right\\}$. Note that we have $x_i^{(p)}= x_i = i$. We combine the projects together to obtain a new dataset $\\mathcal{D} = \\left\\{ \\mathcal{D}^{(p)} \\mid p = 1, ..., P \\right\\}$ (*multi-task learning*). We then have $X = [1, ..., T]^T$ an $(T \\times 1)$ matrix of time indices and $Y = \\left[\\mathbf{y}^{(p)} \\right]_{p=1}^P$ an $(T \\times P)$ matrix of observed values per project. For a given project $p$, we are trying to predict the pledged money $\\mathbf{f}_*  \\equiv \\mathbf{f}_*^{(p)} = \\mathbf{f}_{t:T}^{(p)} = f(X_{t:T}^{(p)})$ at future time indices $X_*  \\equiv X_*^{(p)} = X_{t:T}^{(p)} = X_{t:T} = [t,...,T]^T$ after observing the values $\\mathbf{y}  \\equiv \\mathbf{y}^{(p)} = \\mathbf{y}_{1:t}^{(p)}$ at time indices $X  \\equiv X_{1:t} = [1, ..., t]^T$. To do so in the GP framework, we have\n",
    "\n",
    "$$\\mathbf{f}_* \\mid X, \\mathbf{y}, X_* \\equiv \\mathbf{f}_{t:T}^{(p)} \\mid X_{1:t}, \\mathbf{y}_{1:t}^{(p)}, X_{t:T} \\sim \\mathcal{N} \\left( \\overline{\\mathbf{f}}_{t:T}^{(p)}, \\text{ cov}(\\mathbf{f}_{t:T}^{(p)}) \\right), $$\n",
    "\n",
    "with $\\overline{\\mathbf{f}}_{t:T}^{(p)} \\equiv \\overline{\\mathbf{f}}_*$ as before and $\\text{ cov}(\\mathbf{f}_{t:T}^{(p)}) \\equiv \\text{ cov}(\\mathbf{f}_*)$. The hyperparameters $\\mathbf{\\theta}$ of the GP are learned by maximizing the log marginal likelihood over all the projects, that is\n",
    "\n",
    "$$\\theta_* = \\underset{\\theta} {\\arg\\max} \\sum_{p=1}^P \\log p(\\mathbf{y}^{(p)} \\mid X, \\theta).$$\n",
    "\n",
    "#### Results\n",
    "Again, we couldn't obtain good results with this approach, as the predictions always fall back very quickly to the mean of the GP. **[MORE DETAILS]**\n",
    "\n",
    "### Project Classification\n",
    "#### Model\n",
    "We then decide to try a simpler task. Instead of trying to predict a number of (future) points after some observations, we try now to classify whether a project $p$ will be successful ($c^{(p)} = 1$) or not ($c^{(p)} = 0$). Indeed, by separating the dataset in two classes (*successul* and *failed*), we notice that they have a very different profiles (look at the mean of both classes in [sidekick-classification]) and therefore should be easy to discriminate. To do so, we train one GP using the successful projects, one using the failed projects and try to determine whether a new, partially observed project will be successful or not. We learn $\\theta_s$ the hyperparameters of a GP over $P_s$ *successful* projects only and $\\theta_f$ the hyperparameters of a GP over the $P_f$ *failed* projects. That is, we maximize the log marginal likelihoods\n",
    "\n",
    "$$\\theta_s = \\underset{\\theta} {\\arg\\max} \\sum_{p=1}^{P_s} \\log p(\\mathbf{y}_s^{(p)} \\mid X, \\theta)$$\n",
    "$$\\theta_f = \\underset{\\theta} {\\arg\\max} \\sum_{p=1}^{P_f} \\log p(\\mathbf{y}_f^{(p)} \\mid X, \\theta),$$\n",
    "\n",
    "where $\\mathbf{y}_s$ and $\\mathbf{y}_f$ denote the observations of the successful and failed projects respectively. We then determine the success state $c^{(p)}$ of a new project $p$ with partial observations $\\mathbf{y}_{1:t}^{(p)}$ as\n",
    "\n",
    "$$c_*^{(p)} = \n",
    "\\begin{cases}\n",
    "    1, & \\text{ if } \\log p(\\mathbf{y}_{1:t}^{(p)} \\mid X_{1:t}, \\theta_s) > \\log p(\\mathbf{y}_{1:t}^{(p)} \\mid X_{1:t}, \\theta_f) \\\\\n",
    "    0, & \\text{ otherwise }\n",
    "\\end{cases}.$$\n",
    "\n",
    "#### Results\n",
    "\n",
    "### Outputs as Inputs\n",
    "#### Model\n",
    "We change completely the approach. Instead of using the time as input and trying to predict the output at new time indices, we now consider the pledged money at each time step as input ($\\mathbf{y}$ becomes $\\mathbf{x}$) and the last time index as the output. That is, we have now a dataset $\\mathcal{D} = \\left\\{ (\\mathbf{x}^{(p)}, y^{(p)}) \\mid p = 1, ..., P \\right\\}$ with $\\mathbf{x}^{(p)} = \\mathbf{y}_{1:t}^{(p)}$ and $\\mathbf{y}^{(p)} = y_T^{(p)}$. We then have $X = \\left[\\mathbf{x}^{(p)}\\right]_{p=1}^P$ a $(P \\times t)$ matrix and $\\mathbf{y} = \\left[y_T^{(p)}\\right]_{p=1}^P$ a $(P \\times 1)$ target vector. The difference with the second approach is that now the features for each project are the amount of pledged money at different time step and not the same (shared) input values ($1,...,T$).\n",
    "\n",
    "Our goal now is to predict, for a new project $p$, the final amount of pledged money $f_*^{(p)} = y_T^{(p)} = f(\\mathbf{x}^{(p)}) = f(\\mathbf{y}_{1:t}^{(p)})$ for the money received up to time $t$ $X_*^{(p)} = \\mathbf{y}_{1:t}^{(p)}$ after observing the total pledged money for all projects $\\mathbf{y} = \\left[y_T^{(p)}\\right]_{p=1}^P$ and the money they received up to time $t$, $X = \\left[ \\mathbf{y}_{1:t}^{(p)} \\right]_{p=1}^P$. In the GP framework, we can compute this prediction using\n",
    "\n",
    "$$f_* \\mid X, \\mathbf{y}, X_* \\sim \\mathcal{N}\\left(\\overline{f}_*, \\text{ cov}(f_*)  \\right) \\\\\n",
    "\\overline{f}_* = K(X_*, X) \\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}\\mathbf{y} \\\\\n",
    "\\text{cov}(f_*) = K(X_*, X_*) - K(X_*, X)\\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}K(X, X_*).\n",
    "$$ \n",
    "\n",
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Kernels (as defined in GPy)\n",
    "We assume that each data point $\\mathbf{x}$ have $D$ dimensions.\n",
    "\n",
    "## Linear\n",
    "\n",
    "#### ARD \n",
    "$k(\\mathbf{x}, \\mathbf{x}') = \\sum_{i=1}^{\\text{D}} \\sigma^2_i x_ix_i'$\n",
    "\n",
    "#### Non-ARD \n",
    "$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2_f \\sum_{i=1}^{\\text{D}} x_ix_i' = \\sigma^2_f \\mathbf{x}^T \\mathbf{x}'$ \n",
    "\n",
    "## Polynomial of degree $k$\n",
    "\n",
    "$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2_f(\\sum_{i=1}^{\\text{D}}x_ix_i' + 1)^k = \\sigma^2_f(\\mathbf{x}^T \\mathbf{x}' + 1)^k$\n",
    "\n",
    "## Squared Exponential\n",
    "$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2_f \\text{exp}\\left(\\frac{\\left\\Vert \\mathbf{x}-\\mathbf{x}' \\right\\Vert^2}{2l^2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from numpy.linalg import inv, cholesky\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "\n",
    "def k(xp, xq , l, sigma_f):\n",
    "    \"\"\"Covariance functions with squared exponential of length-scale l and signal noise sigma_f.\"\"\"\n",
    "    #return sigma_f * np.exp(-0.5 * (xp - xq)**2 / float(l**2))\n",
    "    return sigma_f * xp * xq\n",
    "\n",
    "\n",
    "def K(x1, x2, l=1.0, sigma_f=1.0):\n",
    "    \"\"\"Compute the covariance matrix from the observations x.\"\"\"\n",
    "    cov_matrix = np.zeros((len(x1), len(x2)))\n",
    "    for i, p in enumerate(x1):\n",
    "        for j, q in enumerate(x2):\n",
    "            cov_matrix[i, j] = k(p, q, l, sigma_f)\n",
    "    return cov_matrix\n",
    "\n",
    "\n",
    "def gaussian_process_regression(x, y, x_test, k, l=1.0, sigma_n=0.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Computes a regression using Gaussian Process using observations x and y = f(x) and a covariance function k.\n",
    "    \n",
    "    :param x        Indices of observations\n",
    "    :param y        Values at indices x (= f(x))\n",
    "    :param x_test   Indices to get predicted values\n",
    "    :param k        Covariance function\n",
    "    :param sigma_n  Observations noise\n",
    "    :return:        Mean m, variance var and log marginal likelihood lml\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    n_test = len(x_test)\n",
    "    L = cholesky(K(x, x, l, sigma_f) + sigma_n * np.eye(n))\n",
    "    L_inv = inv(L)\n",
    "    a = np.dot(inv(np.transpose(L)), np.dot(L_inv, y))\n",
    "    m = np.dot(K(x_test, x, l, sigma_f), a)  # Predictive mean\n",
    "    v = np.dot(L_inv, K(x, x_test, l, sigma_f))\n",
    "    var = K(x_test, x_test, l, sigma_f) - np.dot(np.transpose(v), v)  # Predictive variance\n",
    "    lml = -0.5 * np.dot(np.transpose(y), a) - np.sum(np.diag(L)) - n * 0.3990899342  # Log maginal likelihood (last term is log2Ï€ / 2)\n",
    "    return m, var + sigma_n * np.eye(n_test), lml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
