{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sidekick - Output as input with non-normalized data\n",
    "\n",
    "#### Model\n",
    "We change completely the approach. Instead of using the time as input and trying to predict the output at new time indices, we now consider the pledged money at each time step as input ($\\mathbf{y}$ becomes $\\mathbf{x}$) and the last time index as the output. That is, we have now a dataset $\\mathcal{D} = \\left\\{ (\\mathbf{x}^{(p)}, y^{(p)}) \\mid p = 1, ..., P \\right\\}$ with $\\mathbf{x}^{(p)} = \\mathbf{y}_{1:t}^{(p)}$ and $\\mathbf{y}^{(p)} = y_T^{(p)}$. We then have $X = \\left[\\mathbf{x}^{(p)}\\right]_{p=1}^P$ a $(P \\times t)$ matrix and $\\mathbf{y} = \\left[y_T^{(p)}\\right]_{p=1}^P$ a $(P \\times 1)$ target vector. The difference with the second approach is that now the features for each project are the amount of pledged money at different time step and not the same (shared) input values ($1,...,T$).\n",
    "\n",
    "Our goal now is to predict, for a new project $p$, the final amount of pledged money $f_*^{(p)} = y_T^{(p)} = f(\\mathbf{x}^{(p)}) = f(\\mathbf{y}_{1:t}^{(p)})$ for the money received up to time $t$ $X_*^{(p)} = \\mathbf{y}_{1:t}^{(p)}$ after observing the total pledged money for all projects $\\mathbf{y} = \\left[y_T^{(p)}\\right]_{p=1}^P$ and the money they received up to time $t$, $X = \\left[ \\mathbf{y}_{1:t}^{(p)} \\right]_{p=1}^P$. In the GP framework, we can compute this prediction using\n",
    "\n",
    "$$f_* \\mid X, \\mathbf{y}, X_* \\sim \\mathcal{N}\\left(\\overline{f}_*, \\text{ cov}(f_*)  \\right) \\\\\n",
    "\\overline{f}_* = K(X_*, X) \\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}\\mathbf{y} \\\\\n",
    "\\text{cov}(f_*) = K(X_*, X_*) - K(X_*, X)\\left[ K(X, X) + \\sigma_n^2I \\right]^{-1}K(X, X_*).\n",
    "$$ \n",
    "\n",
    "As seen in [this plot](http://localhost:8888/notebooks/notebooks/sidekick-1-eda.ipynb#Output-as-input---Global), it seems impossible to do a regression considering all the projects together. However, the two modes corresponding to the successful and failed projects are clearly distinguisable. Therefore we try to perform the regression on each of the two classes separately, using an EM algorithm **[EM DETAILS GO HERE]**\n",
    "\n",
    "#### Results\n",
    "Mixture of (GP) models can probably be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../utils/')) # Add sibling to Python path\n",
    "sys.path.insert(0, os.path.abspath('../src/')) # Add sibling to Python path\n",
    "sys.stdout.flush() # Print output on the fly in Notebook\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12,8)\n",
    "matplotlib.rcParams['font.size'] = 16\n",
    "matplotlib.rcParams['legend.fontsize'] = 16\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import GPy\n",
    "import climin\n",
    "import pickle as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor\n",
    "from dataset import Sidekick\n",
    "from model.utils.progressbar import ProgressBar\n",
    "\n",
    "DATA_DIR = \"../data/sidekick\"\n",
    "\n",
    "def predict_total_pledged(project, t, samples, m_s_test, m_f_test, pi, X_s, y_s, X_f, y_f):\n",
    "    \"\"\"Predict the total pledged money for a given project using a mixture of two GPs:\n",
    "    \n",
    "        p(Y_T | y_{1:t}, theta) = pi * p(y_T | y{1:t}, theta_s) + (1-pi) * p(y_T | y{1:t}, theta_f)\n",
    "        \n",
    "        where pi is the proportion of successful projects. Returns also the classification.\n",
    "    \"\"\"\n",
    "    money = np.expand_dims(project.money[samples], axis=0)\n",
    "    X_observed = np.ndarray(shape=(1, t), buffer=money, dtype=float)\n",
    "    \n",
    "    l_s = m_s_test.rbf.lengthscale\n",
    "    sigma_f_s = m_s_test.rbf.variance\n",
    "    sigma_n_s = m_s_test.Gaussian_noise.variance\n",
    "    mean_s, var_s, likelihood_s = gaussian_process_regression(X_s, y_s, X_observed, k, \n",
    "                                                              l=l_s, \n",
    "                                                              sigma_n=sigma_n_s, \n",
    "                                                              sigma_f=sigma_f_s)\n",
    "    l_f = m_f_test.rbf.lengthscale\n",
    "    sigma_f_f = m_f_test.rbf.variance\n",
    "    sigma_n_f = m_f_test.Gaussian_noise.variance\n",
    "    mean_f, var_f, likelihood_f = gaussian_process_regression(X_f, y_f, X_observed, k, \n",
    "                                                              l=l_f, \n",
    "                                                              sigma_n=sigma_n_f, \n",
    "                                                              sigma_f=sigma_f_f)\n",
    "    y_pred = (mean_s + mean_f) / 2\n",
    "    likelihood_s = gp_log_likelihood(X_observed, y_pred, l_s, sigma_f_s, sigma_n_s)\n",
    "    likelihood_f = gp_log_likelihood(X_observed, y_pred, l_f, sigma_f_f, sigma_n_f)\n",
    "    \n",
    "    print(\"Successful:\", mean_s, var_s, likelihood_s)\n",
    "    print(\"Failed:\", mean_f, var_f, likelihood_f)\n",
    "    print(\"Predicted: %s (%s)\" % (y_pred, \"successful\" if likelihood_s > likelihood_f else \"failed\"))\n",
    "    \n",
    "    #print(likelihood_s)\n",
    "    #print(likelihood_f)\n",
    "    \n",
    "    return likelihood_s > likelihood_f, y_pred\n",
    "\n",
    "def subsample(t, granularity):\n",
    "    if granularity > 1.0 or granularity <= 0:\n",
    "        raise ValueError(\"granularity must be in ]0, 1]\")\n",
    "    t0 = 1\n",
    "    n_samples = int(np.ceil(granularity * t))\n",
    "    if n_samples == 1:\n",
    "        return [t]\n",
    "    else:\n",
    "        return np.linspace(t0, t, n_samples, dtype=int)\n",
    "\n",
    "\n",
    "def k(xp, xq , l, sigma_f):\n",
    "    \"\"\"Covariance functions with squared exponential of length-scale l and signal noise sigma_f.\"\"\"\n",
    "    return sigma_f * np.exp(-0.5 * np.linalg.norm(xp - xq) / float(l**2))\n",
    "    #return sigma_f * xp * xq\n",
    "\n",
    "\n",
    "def K(x1, x2, l=1.0, sigma_f=1.0):\n",
    "    \"\"\"Compute the covariance matrix from the observations x.\"\"\"\n",
    "    cov_matrix = np.zeros((len(x1), len(x2)))\n",
    "    for i, p in enumerate(x1):\n",
    "        for j, q in enumerate(x2):\n",
    "            cov_matrix[i, j] = k(p, q, l, sigma_f)\n",
    "    return cov_matrix\n",
    "\n",
    "def gp_log_likelihood(x, y, l=1.0, sigma_n=0.0, sigma_f=1.0):\n",
    "    n = len(x)\n",
    "    L = np.linalg.cholesky(K(x, x, l, sigma_f) + sigma_n * np.eye(n))\n",
    "    a = sp.linalg.solve_triangular(L.T, sp.linalg.solve_triangular(L, y, lower=True))\n",
    "    lml = -0.5 * np.sum(y * a) - np.sum(np.log(np.diag(L))) - n * 0.918938533205\n",
    "    return lml\n",
    "\n",
    "\n",
    "def gaussian_process_regression(X, y, x_test, k, l=1.0, sigma_n=0.0, sigma_f=1.0):\n",
    "    \"\"\"\n",
    "    Computes a regression using Gaussian Process using observations x and y = f(x) and a covariance function k.\n",
    "    \n",
    "    :param X        Inputs (NxD)\n",
    "    :param y        Values at indices x (= f(x))\n",
    "    :param x_test   Indices to get predicted values\n",
    "    :param k        Covariance function\n",
    "    :param sigma_n  Observations noise\n",
    "    :return:        Mean m, variance var and log marginal likelihood lml\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    n_test = len(x_test)\n",
    "    # Cholesky decompostion\n",
    "    L = np.linalg.cholesky(K(X, X, l, sigma_f) + sigma_n * np.eye(n))\n",
    "    a = sp.linalg.solve_triangular(L.T, sp.linalg.solve_triangular(L, y, lower=True))\n",
    "    k_star = K(X, x_test, l, sigma_f)\n",
    "    # Predictive mean\n",
    "    m = np.dot(k_star.T, a)\n",
    "    # Predictive variance\n",
    "    v = sp.linalg.solve_triangular(L, k_star, lower=True)\n",
    "    var = K(x_test, x_test, l, sigma_f) - np.dot(np.transpose(v), v)\n",
    "    # Log maginal likelihood (last term is log2π / 2)\n",
    "    #lml = -0.5 * np.sum(y * a) - np.sum(np.log(np.diag(L))) - n * 0.918938533205\n",
    "    lml = 0\n",
    "    return m, var + sigma_n * np.eye(n_test), lml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and separate successful from failed projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data set...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "sk = Sidekick(seed=2)\n",
    "sk.load(light=False)\n",
    "# projects_train, projects_test = sk.split()\n",
    "# successful = sk.successful()\n",
    "# failed = sk.failed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GP on all the projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projects_train, projects_test = sk.split(shuffle=False)\n",
    "\n",
    "t = 25\n",
    "granularity = 0.001\n",
    "samples = subsample(t, granularity)\n",
    "n_samples = len(samples)\n",
    "\n",
    "T = 999\n",
    "ARD = True\n",
    "outlier_threshold = 10000\n",
    "\n",
    "projects_train_filtered = [p for p in projects_train if np.all((p.money[T] * p.goal - outlier_threshold) <= 0) and np.all((p.money[samples]* p.goal - outlier_threshold) <= 0)]\n",
    "projects_test_filtered = [p for p in projects_test if np.all((p.money[T] * p.goal - outlier_threshold) <= 0) and np.all((p.money[samples]* p.goal - outlier_threshold) <= 0)]\n",
    "\n",
    "X_train = np.ndarray(shape=(len(projects_train_filtered), n_samples), buffer=np.array([p.money[samples] * p.goal for p in projects_train_filtered]), dtype=float)\n",
    "y_train = np.expand_dims(np.array([p.money[T] * p.goal for p in projects_train_filtered]), axis=1)\n",
    "X_test = np.ndarray(shape=(len(projects_test_filtered), n_samples), buffer=np.array([p.money[samples] * p.goal for p in projects_test_filtered]), dtype=float)\n",
    "y_test = np.expand_dims(np.array([p.money[T] * p.goal for p in projects_test_filtered]), axis=1)\n",
    "\n",
    "#c = 0.1\n",
    "#X_train = np.log(X_train + c)\n",
    "#X_test = np.log(X_test + c) \n",
    "       \n",
    "print(X_train.shape)\n",
    "\n",
    "iterations = 1000\n",
    "scaling = 4000\n",
    "num_inducing_points = 100\n",
    "Z = np.random.rand(num_inducing_points, 1) * scaling # Inducing points\n",
    "batchsize = 100\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=n_samples, ARD=ARD)\n",
    "m = GPy.core.SVGP(X_train, y_train, Z, kernel, GPy.likelihoods.Gaussian(), batchsize=batchsize)\n",
    "#m.kern.white.variance = 1e-5 # Random noise to stabilize the inversion (\"fixed jitter\")\n",
    "#m.kern.white.fix()\n",
    "# + GPy.kern.White(1)\n",
    "\n",
    "opt = climin.Adadelta(m.optimizer_array, m.stochastic_grad, step_rate=0.8, momentum=0.9)\n",
    "from ipywidgets import Text\n",
    "from IPython.display import display\n",
    "\n",
    "text = Text(description=\"Likelihood:\", width=400)\n",
    "display(text)\n",
    "\n",
    "import sys\n",
    "def callback(i):\n",
    "    text.value = str(m.log_likelihood()) + \"(i=%s)\" % i['n_iter']\n",
    "    #Stop after M iterations\n",
    "    if i['n_iter'] > iterations:\n",
    "        print(\"Hitting maximum iteration.\")\n",
    "        return True\n",
    "    return False\n",
    "info = opt.minimize_until(callback)\n",
    "\n",
    "#m = GPy.models.GPRegression(X_train, y_train, kernel)\n",
    "#m.optimize()\n",
    "\n",
    "display(m)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(X_train, y_train, 'kx', alpha=0.2, label=\"Data\")\n",
    "ax.set_xlabel('Money at time %s' % t)\n",
    "ax.set_ylabel('Goal')\n",
    "ax.set_title('SVI Y prediction with data')\n",
    "m.plot(plot_limits=(X_train.min(),X_train.max()), ax=ax)\n",
    "ax.set_xlim((X_train.min(),X_train.max()))\n",
    "plt.legend(['Training Data', 'Mean'], numpoints=1, loc='best')\n",
    "plt.show()\n",
    "\n",
    "if n_samples == 2: \n",
    "    plot = m.plot()\n",
    "    plt.xlabel(\"Money pledged at time %s\" % t)\n",
    "    plt.ylabel(\"Total pldeged money\")\n",
    "    plt.title(\"Total pledged money for each project given the amount at different time\")\n",
    "    #plt.xlim([0, 1.2])\n",
    "    #plt.ylim([0, outlier_threshold])\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(\"gp_output_input.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction for one project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = 1\n",
    "project_test = projects_test_filtered[test]\n",
    "X_observed = np.expand_dims(X_test[test], axis=0)\n",
    "y_pred, var = m.predict(X_observed)\n",
    "\n",
    "#print \"Predicted as %0.4f (%0.4f)\" % (yT_mean, yT_std)\n",
    "print(\"Predicted: %.2f (%.2f)\" % (y_pred[0,0], var[0, 0]))\n",
    "print(\"Actual: %0.2f\" % (y_test[test]))\n",
    "project_test.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.600523168909\n",
      "RMSE:  3017.91106705\n",
      "RMSE successful:  3144.42347884\n",
      "RMSE failed:  0.0\n"
     ]
    }
   ],
   "source": [
    "se_successful = []\n",
    "se_failed = []\n",
    "se_total = []\n",
    "accuracy = 0\n",
    "for i, x_test in enumerate(X_test):\n",
    "    p = projects_test_filtered[i]\n",
    "    goal = float(p.goal)\n",
    "    x_test = np.expand_dims(x_test, axis=0)\n",
    "    y_pred, y_var = m.predict(x_test)\n",
    "    y_pred = y_pred[0, 0]\n",
    "    y_var = y_var[0, 0]\n",
    "    y_actual = y_test[i]\n",
    "    se = (y_pred - y_actual)**2\n",
    "    se_total.append(se)\n",
    "    if y_test[i] >= 1.0:  # Project is successful\n",
    "        se_successful.append(se)\n",
    "    else:  # project is failed\n",
    "        se_failed.append(se)\n",
    "    if (y_pred / goal >= 1 and y_actual / goal >= 1) or (y_pred / goal < 1 and y_actual / goal < 1):\n",
    "        accuracy += 1\n",
    "    #else:\n",
    "    #    print(\"Error:\", i)\n",
    "\n",
    "rmse_successful = np.sqrt(np.mean(se_successful))\n",
    "rmse_failed = np.sqrt(np.mean(se_failed))\n",
    "rmse_total = np.sqrt(np.mean(se_total))\n",
    "accuracy /= float(len(y_test))\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"RMSE: \", rmse_total)\n",
    "print(\"RMSE successful: \", rmse_successful)\n",
    "print(\"RMSE failed: \", rmse_failed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "seed = 2\n",
    "\n",
    "t = 25\n",
    "granularity = 0.001\n",
    "samples = subsample(t, granularity)\n",
    "n_samples = len(samples)\n",
    "T = 999\n",
    "ARD = False\n",
    "outliers_threshold = 10000\n",
    "\n",
    "N_train = int(floor(0.8*N))\n",
    "N_test = N - N_train\n",
    "\n",
    "N_projects = sk.choose_n_projects(n=N)\n",
    "projects_train = N_projects[:N_train]\n",
    "projects_test = N_projects[N_train:]\n",
    "successful = [p for p in projects_train if p.successful and p.money[T] * p.goal < outliers_threshold]\n",
    "failed = [p for p in projects_train if not p.successful and p.money[T] * p.goal < outliers_threshold]\n",
    "pi = len(successful) / float(N_train)\n",
    "\n",
    "print(\"Successful: %s (train)\" % len(successful))\n",
    "print(\"Failed: %s (train)\" % len(failed))\n",
    "print(\"p(successful) = %0.4f (prior)\" % pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GP-RBF on successful projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_s = np.ndarray(shape=(len(successful), n_samples), buffer=np.array([p.money[samples] * p.goal for p in successful]), dtype=float) \n",
    "Y_train_s = np.expand_dims(np.array([p.money[T] * p.goal for p in successful]), axis=1)\n",
    "print(X_train_s.shape)\n",
    "print(Y_train_s.shape)\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=n_samples, ARD=ARD)\n",
    "m_s = GPy.models.GPRegression(X_train_s, Y_train_s, kernel)\n",
    "m_s.optimize()\n",
    "#m_s.rbf.lengthscale = 0.01\n",
    "display(m_s)\n",
    "m_s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view_dimension(d, total):\n",
    "    inputs = range(total)\n",
    "    inputs.pop(d)\n",
    "    fixed_inputs = [(i, 0) for i in inputs]\n",
    "    return fixed_inputs\n",
    "\n",
    "print(m_s.rbf.variance)\n",
    "print(m_s.rbf.lengthscale)\n",
    "f = view_dimension(9, t)\n",
    "fig = m_s.plot(fixed_inputs=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GP-RBF on failed projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_f = np.ndarray(shape=(len(failed), n_samples), buffer=np.array([p.money[samples] * p.goal for p in failed]), dtype=float) \n",
    "Y_train_f = np.expand_dims(np.array([p.money[T] * p.goal for p in failed]), axis=1)\n",
    "print(X_train_f.shape)\n",
    "print(Y_train_f.shape)\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=n_samples, ARD=ARD)\n",
    "m_f = GPy.models.GPRegression(X_train_f, Y_train_f, kernel)\n",
    "m_f.optimize()\n",
    "display(m_f)\n",
    "m_f.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print m_f.rbf.variance\n",
    "print m_f.rbf.lengthscale\n",
    "f = view_dimension(2, t)\n",
    "fig = m_f.plot(fixed_inputs=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_s_test = m_s.copy()\n",
    "m_f_test = m_f.copy()\n",
    "project_test = projects_test[0]\n",
    "\n",
    "success, y_pred = predict_total_pledged(project_test, n_samples, samples, m_s_test, m_f_test, pi, X_train_s, Y_train_s, X_train_f, Y_train_f)\n",
    "#print \"Predicted as %0.4f (%0.4f)\" % (yT_mean, yT_std)\n",
    "correct = (project_test.successful and success) or (not project_test.successful and not success)\n",
    "print(\"Classified as %s, %s\" % (\"successful\" if success else \"failed\", \"CORRECT!\" if correct else \"wrong...\"))\n",
    "print(\"Actual total pledged money: %0.4f\" % (project_test.money[T] * project_test.goal))\n",
    "project_test.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_s_test = m_s.copy()\n",
    "m_f_test = m_f.copy()\n",
    "se_successful = 0\n",
    "std_successful = 0\n",
    "se_failed = 0\n",
    "std_failed = 0\n",
    "loss01 = 0\n",
    "count_classifier_successful = 0\n",
    "total_considered = 0\n",
    "for project in projects_test:\n",
    "    if project.money[T] < outliers_threshold:\n",
    "        total_considered += 1\n",
    "        success, y_pred = predict_total_pledged(project, t, samples, m_s_test, m_f_test, pi, X_train_s, Y_train_s, X_train_f, Y_train_f)\n",
    "        if project.successful: \n",
    "                #se_successful += (project.money[T] - y_T_mean)**2\n",
    "                #std_successful += yT_std\n",
    "                if not success:  # Classification error\n",
    "                    loss01 += 1\n",
    "        else:  # project is failed\n",
    "            #se_failed += (project.money[T] - y_T_mean)**2\n",
    "            #std_failed += yT_std\n",
    "            if success:  # Classification error\n",
    "                loss01 += 1\n",
    "        if success:\n",
    "            count_classifier_successful += 1\n",
    "        \n",
    "#print \"RMSE = %0.4f (±%0.4f)\" % (np.sqrt(np.mean(se_successful)), std_successful / float(N_test))\n",
    "print(\"01-Loss = %0.4f\" % (loss01 / float(N_test)))\n",
    "print(\"Classifed as successful: %s out of %s projects considered\" % (count_classifier_successful, total_considered))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
